{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NollyKeyz/Simple-Python-Projects/blob/main/Williams_Knowledge_Classification_Task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDXfknwvJ4Rz"
      },
      "source": [
        "# This is the code for Natural Language Processing (NLP) Task One\n",
        "\n",
        "###I aim to extract email from my spam folder, preprocess it and use the body of the email for classification task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIycmmfkKxbR"
      },
      "source": [
       ,
        "### Title: Email Inbox Texts Classification Starting with Raw Text and Using the Embedding Space\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "rCH0x5xtikIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NbvvvmvpRgy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "email_document = pd.read_csv('email_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHdsAoQlLq-C"
      },
      "outputs": [],
      "source": [
        "email_document['Subject']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EidUOKCLdyL9"
      },
      "outputs": [],
      "source": [
        "email_document.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aa6PssBet2p"
      },
      "outputs": [],
      "source": [
        "email_document.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voBwaPlHLp7j"
      },
      "source": [
        "# Pre-processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_773ov5w3n7"
      },
      "outputs": [],
      "source": [
        "!pip install contractions\n",
        "\n",
        "import contractions # useful for single words containing certain characters\n",
        "import re # for implementing some of the preprocessing steps\n",
        "from string import punctuation # the punctuation is used to escape punctuation\n",
        "\n",
        "def clean_text(text):\n",
        "    # make text lowercase\n",
        "    text = str(text).lower()\n",
        "    #  remove HTML tags\n",
        "    text = re.sub(r'<[^>]+>', ' ', text)\n",
        "    # remove text in square brackets\n",
        "    text = re.sub('\\[.*?\\]', ' ', text)\n",
        "    # expand contractions especially for words with apostrophe\n",
        "    text = \" \".join([contractions.fix(expanded_word) for expanded_word in text.split()])\n",
        "    # remove links\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', ' ', text)\n",
        "    text = re.sub('<.*?>+', ' ', text)\n",
        "    # remove new lines\n",
        "    text = re.sub('\\n', ' ', text)\n",
        "    # remove words containing numbers\n",
        "    text = re.sub('\\w*\\d\\w*', ' ', text)\n",
        "    # remove punctuation\n",
        "    text = re.sub('[%s]' % re.escape(punctuation), ' ', text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6rwV0p8fOBv"
      },
      "outputs": [],
      "source": [
        "# apply clean text fuction on each email in the training dataset\n",
        "email_document['Clean_Body'] = email_document['Body'].apply(lambda x:clean_text(x))\n",
        "email_document['Clean_Subject'] = email_document['Subject'].apply(lambda x:clean_text(x))\n",
        "\n",
        "email_document.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZZGcrC5iGdI"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# calculate the number of sentences for each email body and\n",
        "email_document['no_body_sentences'] = email_document['Clean_Body'].apply(lambda x: len(sent_tokenize(x)))\n",
        "email_document['no__subject_sentences'] = email_document['Clean_Subject'].apply(lambda x: len(sent_tokenize(x)))\n",
        "\n",
        "email_document.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ColDXeyjuwc"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "# tokenize each of the email into words\n",
        "email_document['email_body_words'] = email_document['Clean_Body'].apply(lambda x:word_tokenize(str(x)))\n",
        "email_document['subject_words'] = email_document['Clean_Subject'].apply(lambda x:word_tokenize(str(x)))\n",
        "\n",
        "email_document.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAGMH3wclCZq"
      },
      "outputs": [],
      "source": [
        "from collections import Counter # used for counting the frequency of words appearance\n",
        "\n",
        "top = Counter([item for sublist in email_document['email_body_words'] for item in sublist])\n",
        "temp_df = pd.DataFrame(top.most_common(40))\n",
        "temp_df.columns = ['Common_words','count']\n",
        "temp_df.style.background_gradient(cmap = 'Blues')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZLYRTQamADA"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords # i imported this to remove stopwords\n",
        "\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaZqUY6Yib-n"
      },
      "outputs": [],
      "source": [
        "stop_words = stopwords.words('english')\n",
        "stop_words.extend(['utf', 'b', 'q'])\n",
        "\n",
        "def remove_stopwords(texts):\n",
        "  return [word for word in texts if word not in stop_words]\n",
        "\n",
        "email_document['email_body_without_sw'] = email_document['email_body_words'].apply(lambda x:remove_stopwords(x))\n",
        "email_document['subject_without_sw'] = email_document['subject_words'].apply(lambda x:remove_stopwords(x))\n",
        "email_document.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2hu0l4vmrO6"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "top = Counter([item for sublist in email_document['email_body_without_sw'] for item in sublist])\n",
        "temp_df = pd.DataFrame(top.most_common(40))\n",
        "temp_df.columns = ['Common_words','count']\n",
        "temp_df.style.background_gradient(cmap = 'Blues')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1vAXiASALv_"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "top = Counter([item for sublist in email_document['subject_without_sw'] for item in sublist])\n",
        "temp_df = pd.DataFrame(top.most_common(100))\n",
        "temp_df.columns = ['Common_words','count']\n",
        "temp_df.style.background_gradient(cmap = 'Reds')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpvpQKyrTIDo"
      },
      "outputs": [],
      "source": [
        "def class_creation(subjects):\n",
        "\n",
        "  # Keywords for identifying different categories\n",
        "  social_keywords = ['facebook', 'email', 'emails', 'linkedin', 'post', 'twitter', 'commented', 'posts', 'page', 'invitation', 'invitations', 'posted', ]\n",
        "  religious_plus_education_keywords = ['proverb', 'school']\n",
        "  finance_keywords = ['transaction', 'gens', 'otp']\n",
        "\n",
        "  # Initialize list to store labelled subjects\n",
        "  result = []\n",
        "\n",
        "  for subject in subjects:\n",
        "    if any(keyword in subject for keyword in social_keywords):\n",
        "      result.append('social')\n",
        "    elif any(keyword in subject for keyword in religious_plus_education_keywords):\n",
        "      result.append('religious and education')\n",
        "    elif any(keyword in subject for keyword in finance_keywords):\n",
        "      result.append('finance')\n",
        "    else:\n",
        "      result.append('other updates')\n",
        "  return result\n",
        "\n",
        "\n",
        "email_document['class_labels'] = class_creation(email_document['subject_without_sw'])\n",
        "email_document.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIPyACoPLxiY"
      },
      "outputs": [],
      "source": [
        "def access_num_label(string_label):\n",
        "  labels = []\n",
        "  for label in string_label:\n",
        "    if label == 'social':\n",
        "      labels.append(int(0))\n",
        "    elif label == 'finance':\n",
        "      labels.append(int(1))\n",
        "    elif label == 'religious and education':\n",
        "      labels.append(int(2))\n",
        "    elif label == 'other updates':\n",
        "      labels.append(int(3))\n",
        "  return labels\n",
        "\n",
        "email_document['label'] = access_num_label(email_document['class_labels'])\n",
        "email_document.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySIgNsHDy2Pv"
      },
      "outputs": [],
      "source": [
        "nltk.download(\"wordnet\")\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "email_document['lemmatized_body'] = email_document['email_body_without_sw'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
        "email_document['lemmatized_subject'] = email_document['subject_without_sw'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
        "email_document.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pC4scyKA1GCx"
      },
      "outputs": [],
      "source": [
        "email_document['final_body'] = email_document['lemmatized_body'].apply(lambda x:' '.join(x))\n",
        "email_document['final_subject'] = email_document['lemmatized_subject'].apply(lambda x:' '.join(x))\n",
        "email_document.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tykdbq5zBx2v"
      },
      "source": [
        "#**Embedding Methods**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2ajIsVxEKXG"
      },
      "source": [
        "##**Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afVyO4Fz2oXR"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow_hub\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-cache search nvidia"
      ],
      "metadata": {
        "id": "ZFAGdlNdDHAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eW9u7AIEYVF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pUq53qtwYfa"
      },
      "outputs": [],
      "source": [
        "#import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "batch_size = 128\n",
        "# Split the data into training (40%), validation (30%), and testing (30%) sets\n",
        "train_data, temp_data = train_test_split(email_document, test_size=0.6, random_state=42)\n",
        "valid_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
        "\n",
        "\n",
        "# Assuming train_data, valid_data, and test_data contain your training, validation, and testing data respectively\n",
        "train_examples, train_labels = train_data['final_body'], train_data['label']\n",
        "valid_examples, valid_labels = valid_data['final_body'], test_data['label']\n",
        "test_examples, test_labels = test_data['final_body'], test_data['label']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "\n",
        "train_examples_list = list(train_examples)\n",
        "train_labels_list = list(train_labels)\n",
        "\n",
        "valid_examples_list = list(valid_examples)\n",
        "valid_labels_list = list(valid_labels)\n",
        "\n",
        "test_examples_list = list(test_examples)\n",
        "test_labels_list = list(test_labels)"
      ],
      "metadata": {
        "id": "ACVqjEhY9G-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_examples_list)"
      ],
      "metadata": {
        "id": "wZ4AnIbv9ZB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "182sf_7ssB1J"
      },
      "outputs": [],
      "source": [
        "train_data_np = np.array(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_labels_list)"
      ],
      "metadata": {
        "id": "VCjDZnJPC_Cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdJkoUlMMxXR"
      },
      "outputs": [],
      "source": [
        "print(\"Training entries: {}, test entries: {}\".format(len(train_examples), len(test_examples)))\n",
        "print(type(train_examples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsWSStDlD8U4"
      },
      "outputs": [],
      "source": [
        "# Convert the lists to a NumPy arrays\n",
        "train_examples_np = np.array(train_examples)\n",
        "train_labels_np = np.array(train_labels)\n",
        "\n",
        "valid_examples_np = np.array(valid_examples)\n",
        "valid_labels_np = np.array(valid_labels)\n",
        "\n",
        "test_examples_np = np.array(test_examples)\n",
        "test_labels_np = np.array(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJuzrK22NmhA"
      },
      "outputs": [],
      "source": [
        "print(type(train_examples_np))\n",
        "print(type(train_labels_np))\n",
        "\n",
        "print(type(valid_examples_np))\n",
        "print(type(valid_labels_np))\n",
        "\n",
        "print(type(test_examples_np))\n",
        "print(type(test_labels_np))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azjdrRvhehkM"
      },
      "outputs": [],
      "source": [
        "train_examples_np[:10]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_examples_list[:10]"
      ],
      "metadata": {
        "id": "bpw7Vu9JBVw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRE0F2P2HTxP"
      },
      "outputs": [],
      "source": [
        "train_labels_np[:10]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels_list[:10]"
      ],
      "metadata": {
        "id": "mhe_ZctzBc_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHwlX15VNAo1"
      },
      "source": [
        "#**Model Building**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwcEmp3aCHmX"
      },
      "source": [
        "## **First Method**\n",
        "\n",
        "###nnlm-en-dim50 Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXjzBJAGB0zX"
      },
      "outputs": [],
      "source": [
        "model_1 = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
        "hub_layer = hub.KerasLayer(model_1, input_shape=[], dtype=tf.string, trainable=True)\n",
        "hub_layer(train_examples_np[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oK1k2dqFt6p"
      },
      "outputs": [],
      "source": [
        "num_classes = 4 #to handle multiple (4) classes\n",
        "\n",
        "model_1 = tf.keras.Sequential()\n",
        "model_1.add(hub_layer)\n",
        "model_1.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "model_1.add(tf.keras.layers.Dense(num_classes, activation='relu'))\n",
        "\n",
        "model_1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tKaHCCthxOI"
      },
      "outputs": [],
      "source": [
        "model_1.compile(optimizer='adam',\n",
        "              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuFwRCIHRroK"
      },
      "outputs": [],
      "source": [
        "history = model_1.fit(train_examples_np,\n",
        "                    train_labels_np,\n",
        "                    epochs=20,\n",
        "                    batch_size=512,\n",
        "                    validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1ll2xUhj6Ni"
      },
      "outputs": [],
      "source": [
        "results = model_1.evaluate(test_examples_np, test_labels_np)\n",
        "\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWm0oQpKlV6A"
      },
      "outputs": [],
      "source": [
        "y_pred = model_1.predict(test_examples_np)\n",
        "classes_x=np.argmax(y_pred ,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGYZmzOpJ0F8"
      },
      "outputs": [],
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_96BfyzSJ2Bs"
      },
      "outputs": [],
      "source": [
        "acc = history_dict['accuracy_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9LFJme-TJmj"
      },
      "outputs": [],
      "source": [
        "plt.clf()   # clear figure\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3TrzYlKvEjS"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, f1_score, recall_score, precision_score\n",
        "import seaborn as sns\n",
        "# Evaluate the model on the test set\n",
        "y_pred = model_1.predict(test_examples_np)\n",
        "y_pred_labels = y_pred.argmax(axis=1)\n",
        "y_test_labels = test_labels_np\n",
        "\n",
        "# Calculate confusion matrix\n",
        "confusion_mat = confusion_matrix(y_test_labels, y_pred_labels)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "sns.heatmap(confusion_mat,\n",
        "            annot = True,\n",
        "            cmap=plt.cm.Blues)\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(y_test_labels, y_pred_labels, average='weighted')\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "# Calculate recall\n",
        "recall = recall_score(y_test_labels, y_pred_labels, average='weighted')\n",
        "print(\"Recall:\", recall)\n",
        "\n",
        "# Calculate precision\n",
        "precision = precision_score(y_test_labels, y_pred_labels, average='weighted')\n",
        "print(\"Precision:\", precision)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DumA5lEZEVtC"
      },
      "outputs": [],
      "source": [
        "# Plot the learning curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(np.arange(1, len(history.history['sparse_categorical_accuracy']) + 1), history.history['sparse_categorical_accuracy'], label='Training acc')\n",
        "plt.plot(np.arange(1, len(history.history['val_sparse_categorical_accuracy']) + 1), history.history['val_sparse_categorical_accuracy'], label='Validation acc')\n",
        "plt.title('Learning Curve')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hPLqRqnrJNf"
      },
      "source": [
        "## **Second Method**\n",
        "\n",
        "## nnlm-en-dim50-with-normalization/2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8bmpq6lRazg"
      },
      "outputs": [],
      "source": [
        "model_2 = \"https://tfhub.dev/google/nnlm-en-dim50-with-normalization/2\"\n",
        "hub_layer = hub.KerasLayer(model_3, input_shape=[], dtype=tf.string, trainable=True)\n",
        "hub_layer(train_examples_np[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3SaF-XjRv02"
      },
      "outputs": [],
      "source": [
        "num_classes = 4 #to handle multiple (4) classes\n",
        "\n",
        "model_2 = tf.keras.Sequential()\n",
        "model_2.add(hub_layer)\n",
        "model_2.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "model_2.add(tf.keras.layers.Dense(num_classes, activation='relu'))\n",
        "\n",
        "model_2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCEUiLWQr1T0"
      },
      "outputs": [],
      "source": [
        "model_2.compile(optimizer='adam',\n",
        "              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=[tf.metrics.SparseCategoricalAccuracy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njGTj5uxr68N"
      },
      "outputs": [],
      "source": [
        "history = model_2.fit(train_examples_np,\n",
        "                    train_labels_np,\n",
        "                    epochs=10,\n",
        "                    batch_size=512,\n",
        "                    validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwLKI-JWsBLC"
      },
      "outputs": [],
      "source": [
        "results = model_2.evaluate(test_examples_np, test_labels_np)\n",
        "\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jIiyyFxsI2R"
      },
      "outputs": [],
      "source": [
        "results_pred = model_2.predict(test_examples_np)\n",
        "classes_x=np.argmax(results_pred ,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-lHpPYlsOno"
      },
      "outputs": [],
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3TwCcrOsUqm"
      },
      "outputs": [],
      "source": [
        "acc = history_dict['sparse_categorical_accuracy']\n",
        "val_acc = history_dict['val_sparse_categorical_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMEhbHY2sbGa"
      },
      "outputs": [],
      "source": [
        "plt.clf()   # clear figure\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQU5GrBJPEym"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, f1_score, recall_score, precision_score\n",
        "# Evaluate the model on the test set\n",
        "y_pred = model_2.predict(test_examples_np)\n",
        "y_pred_labels = y_pred.argmax(axis=1)\n",
        "y_test_labels = test_labels_np\n",
        "\n",
        "# Calculate confusion matrix\n",
        "confusion_mat = confusion_matrix(y_test_labels, y_pred_labels)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(y_test_labels, y_pred_labels, average='weighted')\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "# Calculate recall\n",
        "recall = recall_score(y_test_labels, y_pred_labels, average='weighted')\n",
        "print(\"Recall:\", recall)\n",
        "\n",
        "# Calculate precision\n",
        "precision = precision_score(y_test_labels, y_pred_labels, average='weighted')\n",
        "print(\"Precision:\", precision)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sClyypXLPXhQ"
      },
      "outputs": [],
      "source": [
        "# Plot the learning curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(np.arange(1, len(history.history['sparse_categorical_accuracy']) + 1), history.history['sparse_categorical_accuracy'], label='Training acc')\n",
        "plt.plot(np.arange(1, len(history.history['val_sparse_categorical_accuracy']) + 1), history.history['val_sparse_categorical_accuracy'], label='Validation acc')\n",
        "plt.title('Learning Curve')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ed0ianW0PiwJ"
      },
      "source": [
        "## **Third Method**\n",
        "\n",
        "### nnlm-en-dim128-with-normalization/2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUZ5rUmisiBo"
      },
      "outputs": [],
      "source": [
        "model_3 = \"https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2\"\n",
        "hub_layer = hub.KerasLayer(model_3, input_shape=[], dtype=tf.string, trainable=True)\n",
        "hub_layer(train_examples_np[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmImim5Usuk_"
      },
      "outputs": [],
      "source": [
        "num_classes = 4 #to handle multiple (4) classes\n",
        "\n",
        "model_3 = tf.keras.Sequential()\n",
        "model_3.add(hub_layer)\n",
        "model_3.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "model_3.add(tf.keras.layers.Dense(num_classes, activation='relu'))\n",
        "\n",
        "model_3.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBV6wOrgtZJ0"
      },
      "outputs": [],
      "source": [
        "model_3.compile(optimizer='adam',\n",
        "              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=[tf.metrics.SparseCategoricalAccuracy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcYYJMTFteiO"
      },
      "outputs": [],
      "source": [
        "history = model_3.fit(train_examples_np,\n",
        "                    train_labels_np,\n",
        "                    epochs=10,\n",
        "                    batch_size=512,\n",
        "                    validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJ02Hd93tktN"
      },
      "outputs": [],
      "source": [
        "results = model_3.evaluate(test_examples_np, test_labels_np)\n",
        "\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqOXDccCtpwD"
      },
      "outputs": [],
      "source": [
        "results_pred = model_3.predict(test_examples_np)\n",
        "classes_x=np.argmax(results_pred ,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGSjeNZSt0er"
      },
      "outputs": [],
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPmQnPUit542"
      },
      "outputs": [],
      "source": [
        "acc = history_dict['sparse_categorical_accuracy']\n",
        "val_acc = history_dict['val_sparse_categorical_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGvRqR_YuAqb"
      },
      "outputs": [],
      "source": [
        "plt.clf()   # clear figure\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKMb90sxuEhz"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, f1_score, recall_score, precision_score\n",
        "# Evaluate the model on the test set\n",
        "y_pred = model_2.predict(test_examples_np)\n",
        "y_pred_labels = y_pred.argmax(axis=1)\n",
        "y_test_labels = test_labels_np\n",
        "\n",
        "# Calculate confusion matrix\n",
        "confusion_mat = confusion_matrix(y_test_labels, y_pred_labels)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(y_test_labels, y_pred_labels, average='weighted')\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "# Calculate recall\n",
        "recall = recall_score(y_test_labels, y_pred_labels, average='weighted')\n",
        "print(\"Recall:\", recall)\n",
        "\n",
        "# Calculate precision\n",
        "precision = precision_score(y_test_labels, y_pred_labels, average='weighted')\n",
        "print(\"Precision:\", precision)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-QOfUgYVHJe"
      },
      "outputs": [],
      "source": [
        "# Plot the learning curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(np.arange(1, len(history.history['sparse_categorical_accuracy']) + 1), history.history['sparse_categorical_accuracy'], label='Training acc')\n",
        "plt.plot(np.arange(1, len(history.history['val_sparse_categorical_accuracy']) + 1), history.history['val_sparse_categorical_accuracy'], label='Validation acc')\n",
        "plt.title('Learning Curve')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lU-07GQIO2so"
      },
      "source": [
        "**BERT Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlL2WQJQO0om"
      },
      "outputs": [],
      "source": [
        "# Load BERT model from TensorFlow Hub\n",
        "bert_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\"\n",
        "bert_layer = hub.KerasLayer(bert_url, trainable=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert-tokenizer"
      ],
      "metadata": {
        "id": "jRyvfYj0sseT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFBertModel, BertTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define parameters\n",
        "batch_size = 16  # Reduced batch size\n",
        "epochs = 2  # Reduced number of epochs\n",
        "\n",
        "# Load the pre-trained BERT model\n",
        "bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Ignore warning about unused weights\n",
        "import logging\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "\n",
        "# Assuming train_data, valid_data, and test_data contain your training, validation, and testing data respectively\n",
        "train_data, temp_data = train_test_split(email_document, test_size=0.6, random_state=42)\n",
        "valid_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
        "\n",
        "# Define train, validation, and test examples and labels\n",
        "train_examples, train_labels = train_data['final_body'], train_data['label']\n",
        "valid_examples, valid_labels = valid_data['final_body'], test_data['label']\n",
        "test_examples, test_labels = test_data['final_body'], test_data['label']\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "train_tokenized_data = tokenizer(list(train_examples), padding=True, truncation=True, return_tensors=\"tf\")\n",
        "valid_tokenized_data = tokenizer(list(valid_examples), padding=True, truncation=True, return_tensors=\"tf\")\n",
        "\n",
        "# Define loss function and optimizer\n",
        "loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "\n",
        "# Compile the model\n",
        "bert_model.compile(optimizer=optimizer,\n",
        "              loss=loss_function,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "history = bert_model.fit(\n",
        "    x=train_tokenized_data,\n",
        "    y=train_labels,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(valid_tokenized_data, valid_labels),\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "-Rj1csTZhfhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 4\n",
        "\n",
        "# Get pooled output and sequence output\n",
        "pooled_output, sequence_output = bert_layer([input_ids_tensor, input_masks_tensor, segment_ids_tensor])\n",
        "\n",
        "# Define additional layers\n",
        "dense_layer = tf.keras.layers.Dense(256, activation='relu')(pooled_output)\n",
        "num_classes = 4  # Example number of output classes\n",
        "output = tf.keras.layers.Dense(num_classes, activation='softmax')(dense_layer)\n",
        "\n",
        "# Create the model\n",
        "model = tf.keras.Model(inputs=[input_ids_tensor, input_masks_tensor, segment_ids_tensor], outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "adoSaVVqcRqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the validation text data\n",
        "valid_tokenized_data = tokenizer(list(valid_examples), padding=True, truncation=True, return_tensors=\"tf\")\n",
        "\n",
        "# Extract input_ids, input_masks, and segment_ids for validation data\n",
        "valid_input_ids = valid_tokenized_data[\"input_ids\"]\n",
        "valid_input_masks = valid_tokenized_data[\"attention_mask\"]\n",
        "valid_segment_ids = valid_tokenized_data[\"token_type_ids\"]\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Define input layers\n",
        "valid_input_ids_tensor = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, batch_size=batch_size, name=\"input_ids\")\n",
        "valid_input_masks_tensor = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, batch_size=batch_size, name=\"attention_mask\")\n",
        "valid_segment_ids_tensor = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, batch_size=batch_size, name=\"token_type_ids\")\n"
      ],
      "metadata": {
        "id": "jjHjRBsraj5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, TFBertModel\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Tokenize the text data\n",
        "train_tokenized_data = tokenizer(list(train_examples), padding=True, truncation=True, return_tensors=\"tf\")\n",
        "valid_tokenized_data = tokenizer(list(valid_examples), padding=True, truncation=True, return_tensors=\"tf\")\n",
        "\n",
        "# Extract input_ids, attention_mask, and token_type_ids\n",
        "input_ids = train_tokenized_data[\"input_ids\"]\n",
        "input_masks = train_tokenized_data[\"attention_mask\"]\n",
        "segment_ids = train_tokenized_data[\"token_type_ids\"]\n",
        "\n",
        "valid_input_ids = valid_tokenized_data[\"input_ids\"]\n",
        "valid_input_masks = valid_tokenized_data[\"attention_mask\"]\n",
        "valid_segment_ids = valid_tokenized_data[\"token_type_ids\"]\n",
        "\n",
        "# Define input layers\n",
        "input_ids_tensor_tf = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"input_ids\")\n",
        "input_masks_tensor_tf = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"attention_mask\")\n",
        "segment_ids_tensor_tf = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"token_type_ids\")\n",
        "\n",
        "# Load the pre-trained BERT model\n",
        "bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Get pooled output and sequence output\n",
        "pooled_output, sequence_output = bert_model([input_ids_tensor_tf, input_masks_tensor_tf, segment_ids_tensor_tf])\n"
      ],
      "metadata": {
        "id": "Hb7Q2GpkO7_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(input_ids))"
      ],
      "metadata": {
        "id": "VSpsgN9MSbho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Get pooled output and sequence output\n",
        "pooled_output, sequence_output = bert_model([input_ids_tensor, input_masks_tensor, segment_ids_tensor])\n",
        "\n",
        "# Additional layers\n",
        "dense_layer = tf.keras.layers.Dense(256, activation='relu')(pooled_output)\n",
        "output_layer = tf.keras.layers.Dense(num_classes, activation='softmax', name='output')(dense_layer)\n",
        "\n",
        "# Create the model\n",
        "model = tf.keras.Model(inputs=[input_ids_tensor, input_masks_tensor, segment_ids_tensor], outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(\n",
        "    x=[input_ids, input_masks, segment_ids],\n",
        "    y=train_labels_np,\n",
        "    batch_size=batch_size,\n",
        "    epochs=10,\n",
        "    validation_data=([valid_input_ids, valid_input_masks, valid_segment_ids], valid_labels_np)"
      ],
      "metadata": {
        "id": "NTp6FXvCQoKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_dict = {\"input_ids\": input_ids, \"attention_mask\": input_masks, \"token_type_ids\": segment_ids}\n",
        "items_list = list(my_dict.items())\n",
        "\n",
        "dict2 ={\"input_ids\": valid_input_ids, \"attention_mask\": valid_input_masks, \"token_type_ids\": valid_segment_ids}\n",
        "valid = list(dict2.items())\n",
        "\n",
        "\n",
        "# Separate inputs and labels from items_list\n",
        "inputs_list = [item[0] for item in items_list]\n",
        "labels_list = [item[1] for item in items_list]\n",
        "\n",
        "\n",
        "# Convert input data and labels to numpy arrays\n",
        "inputs_list_np = np.array([item[1] for item in items_list])\n",
        "labels_list_np = np.array([item[1] for item in items_list])\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(\n",
        "    x=inputs_list_np,\n",
        "    y=labels_list_np,\n",
        "    batch_size=batch_size,\n",
        "    epochs=10,\n",
        "    validation_split=0.2  # Example of adding validation data\n",
        ")\n"
      ],
      "metadata": {
        "id": "69AYBfHyiBAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply BERT layer\n",
        "pooled_output, _ = hub_layer([input_ids_input, input_masks_input, segment_ids_input])\n",
        "\n",
        "# Add dense layer\n",
        "dense_layer = tf.keras.layers.Dense(256, activation='relu')(pooled_output)\n",
        "\n",
        "# Output layer\n",
        "num_classes = 4  # Example number of output classes\n",
        "output = tf.keras.layers.Dense(num_classes, activation='softmax')(dense_layer)"
      ],
      "metadata": {
        "id": "WZVSriGGZx5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pMLzB4QAdsOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = tf.keras.Model(inputs=[input_ids_input, input_masks_input, segment_ids_input], outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "EZDSgIBCZ6H4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert input data to tensors\n",
        "input_ids_tensor = tf.convert_to_tensor(input_ids)\n",
        "input_masks_tensor = tf.convert_to_tensor(input_masks)\n",
        "segment_ids_tensor = tf.convert_to_tensor(segment_ids)\n"
      ],
      "metadata": {
        "id": "T8SajyAhaIKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input IDs Tensor:\", input_ids_tensor)\n",
        "print(\"Input Masks Tensor:\", input_masks_tensor)\n",
        "print(\"Segment IDs Tensor:\", segment_ids_tensor)"
      ],
      "metadata": {
        "id": "Zif-Q04Xbnb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Fit the model\n",
        "history = model.fit(\n",
        "    {\"input_ids\": input_ids_tensor, \"attention_mask\": input_masks_tensor, \"token_type_ids\": segment_ids_tensor},\n",
        "    train_labels_list,\n",
        "    epochs=10,\n",
        "    batch_size=batch_size,\n",
        "    validation_data=({\"input_ids\": valid_input_ids, \"attention_mask\": valid_input_masks, \"token_type_ids\": valid_segment_ids}, valid_labels_list)\n",
        ")"
      ],
      "metadata": {
        "id": "1Q4VPoxabdPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8_r2SLNl3R0"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/tensorflow/models.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZV_4-e03mFK2"
      },
      "outputs": [],
      "source": [
        "%cd models/official\n",
        "!pip install -q tensorflow-text\n",
        "!pip install -q tf-models-official"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXAWxPwokbUe"
      },
      "outputs": [],
      "source": [
        "from official.nlp import optimization  # TensorFlow Model Garden\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenized_data = tokenizer(email_document, padding=True, truncation=True, return_tensors=\"tf\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract input_ids, input_masks, and segment_ids\n",
        "input_ids = tokenized_data[\"input_ids\"]\n",
        "input_masks = tokenized_data[\"attention_mask\"]  # This is the same as input_mask in BERT\n",
        "segment_ids = tokenized_data[\"token_type_ids\"]  # This is the same as segment_ids in BERT\n",
        "\n",
        "batch_size = 128\n",
        "input_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, batch_size=batch_size, name=\"input_ids\")\n",
        "input_masks = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, batch_size=batch_size, name=\"input_masks\")\n",
        "segment_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, batch_size=batch_size, name=\"segment_ids\")\n",
        "\n",
        "print(\"Input IDs:\", input_ids)\n",
        "print(\"Input Masks:\", input_masks)\n",
        "print(\"Segment IDs:\", segment_ids)\n",
        "\n"
      ],
      "metadata": {
        "id": "UcBQW5Aoyw4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPk9pAOVXyPB"
      },
      "outputs": [],
      "source": [
        "# Define your model architecture\n",
        "pooled_output, _ = hub_layer([input_ids, input_masks, segment_ids, ])\n",
        "dense_layer = tf.keras.layers.Dense(256, activation='relu')(pooled_output)\n",
        "num_classes = 4  # Example number of output classes\n",
        "output = tf.keras.layers.Dense(num_classes, activation='softmax', batch_size=batch_size)(dense_layer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Gq6HrcQWQc0"
      },
      "outputs": [],
      "source": [
        "# Create the model\n",
        "model = tf.keras.Model(inputs=[input_ids, input_masks, segment_ids], outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BDHAtFuUC77"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    {\"input_ids\": input_ids_list, \"attention_mask\": input_masks_list, \"token_type_ids\": segment_ids_list},\n",
        "    train_labels_list,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    validation_data=({\"input_ids\": valid_input_ids_list, \"attention_mask\": valid_input_masks_list, \"token_type_ids\": valid_segment_ids_list}, valid_labels_list)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    {\"input_ids\": input_ids_tensor, \"attention_mask\": input_masks_tensor, \"token_type_ids\": segment_ids_tensor},\n",
        "    train_labels_list,\n",
        "    epochs=10,\n",
        "    batch_size=batch_size,\n",
        "    validation_data=({\"input_ids\": valid_input_ids_tensor, \"attention_mask\": valid_input_masks_tensor, \"token_type_ids\": valid_segment_ids_tensor}, valid_labels_list)\n",
        ")"
      ],
      "metadata": {
        "id": "QWj_IEhpcT3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qy1hxWeI7MrF"
      },
      "source": [
        "**THANK YOU**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
